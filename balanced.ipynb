{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0dfb23e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9989fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "from scipy.stats import pointbiserialr, chi2_contingency, uniform, randint\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0e53a",
   "metadata": {},
   "source": [
    "### Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46228771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Data\n",
    "train_file = \"\"\n",
    "df = pd.read_excel(train_file)\n",
    "df.set_index('CID', inplace= True)\n",
    "\n",
    "X, y = df.iloc[:, :-1], df.iloc[:,-1]\n",
    "df_train_x, df_val_x, y_train, y_val = train_test_split(X, y, test_size= 0.1, random_state= 42, stratify= y)\n",
    "\n",
    "print(\"\\n\\033[1mTraining Data information: \\033[0m\\n\")\n",
    "df_train_x.info()\n",
    "print(\"\\n\\033[1mTraining Label information: \\033[0m\\n\")\n",
    "y_train.info()\n",
    "label_count_tr = y_train.value_counts()\n",
    "print(\"\\n\\033[1mIn the Training Dataset:\\033[0m\\n\")\n",
    "print(f\"Percentage of positively labeled data: {(label_count_tr[1] / (label_count_tr[0] + label_count_tr[1])) * 100}%\")\n",
    "\n",
    "print(\"\\n\\033[1mValidation Data information: \\033[0m\\n\")\n",
    "df_val_x.info()\n",
    "print(\"\\n\\033[1mValidation Label information: \\033[0m\\n\")\n",
    "y_val.info()\n",
    "label_count_ts = y_val.value_counts()\n",
    "print(\"\\n\\033[1mIn the Validation Dataset: \\033[0m\\n\")\n",
    "print(f\"Percentage of positively labeled data: {(label_count_ts[1] / (label_count_ts[0] + label_count_ts[1])) * 100}%\")\n",
    "\n",
    "# Test Data\n",
    "test_file = \"\"\n",
    "df_test = pd.read_excel(test_file)\n",
    "df_test.set_index('CID', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b915b",
   "metadata": {},
   "source": [
    "### Pipeline Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_to_drop):\n",
    "        self.columns_to_drop = col_to_drop\n",
    "    \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns= self.columns_to_drop)\n",
    "\n",
    "class OneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, binary= False):\n",
    "        self.columns = columns\n",
    "        self.binary = binary\n",
    "        self.encoders = {}\n",
    "        self.new_column_names = {}\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            if self.binary:                \n",
    "                category_names = sorted(list(X_transformed[column].drop_duplicates()))\n",
    "                self.binary_check(column, category_names)\n",
    "                self.encoders[column] = {col: id for id, col in enumerate(category_names)}\n",
    "            else:\n",
    "                category_names = sorted([x for x in X_transformed[column].drop_duplicates() \n",
    "                                         if not (isinstance(x, float) and math.isnan(x))])\n",
    "                self.new_column_names[column] = [f\"{column}_{c}\" for c in category_names]\n",
    "                encoder = OneHotEncoder(dtype= np.int64, drop= 'if_binary')\n",
    "                encoder.fit(X_transformed[[column]])\n",
    "                self.encoders[column] = encoder\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            if self.binary:\n",
    "                X_transformed[column] = X_transformed[column].map(self.encoders[column])\n",
    "            else:\n",
    "                ohe_matrix = self.encoders[column].transform(X_transformed[[column]]).toarray()\n",
    "                if self.has_nan(X_transformed[column]):\n",
    "                    ohe_matrix = ohe_matrix[:, :-1]\n",
    "                for i, new_col in enumerate(self.new_column_names[column]):\n",
    "                    X_transformed.insert(X_transformed.columns.get_loc(column) + i, new_col, ohe_matrix[:, i])\n",
    "                X_transformed.drop(columns= [column], inplace= True)\n",
    "        return X_transformed\n",
    "    \n",
    "    def binary_check(self, column, categories):\n",
    "        if self.has_nan(categories):\n",
    "            raise ValueError(f\"Can't perform binary encoding to column {column} because there are NaN values.\")\n",
    "        if len(categories) > 2 or len(categories) == 0:\n",
    "            raise ValueError(f\"Can't perform binary encoding to column {column} because the number of categories to binary encode is wrong.\")\n",
    "\n",
    "    def has_nan(self, value_list):\n",
    "        return any([math.isnan(x) for x in value_list if isinstance(x, float)])\n",
    "\n",
    "class TargetMeanEncoding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, y, m= 10):\n",
    "        self.columns = columns\n",
    "        self.y_target = y\n",
    "        self.m = m\n",
    "        self.encoding_dict = {}\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        global_mean = self.y_target.mean()\n",
    "        for column in self.columns:\n",
    "            smoothed_mean = self.smooth_mean(X[column], global_mean)\n",
    "            self.encoding_dict[column] = dict(zip(X[column].dropna().drop_duplicates(), smoothed_mean.dropna().drop_duplicates()))        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        global_mean = self.y_target.mean()\n",
    "        \n",
    "        for column in self.columns:\n",
    "            if column in self.encoding_dict:\n",
    "                X_transformed[column] = X_transformed[column].map(self.encoding_dict[column]).fillna(global_mean)\n",
    "            else:\n",
    "                X_transformed[column] = X_transformed[column].fillna(global_mean)\n",
    "                \n",
    "        return X_transformed\n",
    "    \n",
    "    def smooth_mean(self, values, mean):\n",
    "        encoded_mean = self.y_target.groupby(values).mean()\n",
    "        counts = values.map(values.value_counts())\n",
    "        return (values.map(encoded_mean) * counts + mean * self.m) / (counts + self.m)\n",
    "    \n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, scaler_cls, **scaler_params):\n",
    "        self.columns = columns\n",
    "        self.scaler_cls = scaler_cls\n",
    "        self.scaler_params = scaler_params\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        for col in self.columns:\n",
    "            scaler = self.scaler_cls(**self.scaler_params)\n",
    "            scaler.fit(X[[col]])\n",
    "            self.scalers[col] = scaler\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_copy[col] = self.scalers[col].transform(X_copy[[col]])\n",
    "        return X_copy\n",
    "\n",
    "class PolynomialFeature(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, degree= 2):\n",
    "        self.columns = columns\n",
    "        self.degree = degree\n",
    "\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            for power in range(2, self.degree + 1):\n",
    "                new_col_name = f\"{col}_pow{power}\"\n",
    "                X_copy[new_col_name] = X_copy[col] ** power\n",
    "        return X_copy\n",
    "\n",
    "class LogTransformFeature(BaseEstimator, TransformerMixin):\n",
    "            \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            new_col_name = f\"{col}_log\"\n",
    "            X_copy[new_col_name] = np.log1p(X_copy[col])\n",
    "        return X_copy\n",
    "            \n",
    "class InteractionFeature(BaseEstimator, TransformerMixin):\n",
    "            \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for (col1, col2) in combinations(self.columns, 2):\n",
    "            new_col_name = f\"{col1}_x_{col2}\"\n",
    "            X_copy[new_col_name] = X_copy[col1] * X_copy[col2]\n",
    "        return X_copy\n",
    "        \n",
    "class BinningFeature(BaseEstimator, TransformerMixin):\n",
    "            \n",
    "    def __init__(self, columns, bins= 5):\n",
    "        self.columns = columns\n",
    "        self.bins = bins\n",
    "\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            new_col_name = f\"{col}_binned\"\n",
    "            X_copy[new_col_name], _ = pd.cut(X_copy[col], bins= self.bins, retbins= True, labels= False)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12900d",
   "metadata": {},
   "source": [
    "### Investigating Features\n",
    "##### Examining Correlations of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eea8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = {}\n",
    "for feature in df.columns[12:-1]:\n",
    "    unique_categories[feature] = X[feature].nunique()\n",
    "    \n",
    "for feature, count in unique_categories.items():\n",
    "    print(f\"Feature {feature} has {count} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a14c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pipeline = Pipeline([\n",
    "    (\"Binary Encoder\", OneHotEncoding([\"A\"], binary= True)),\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([\"B\", \"C\", \"D\"])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([\"E\"], y_train))\n",
    "])\n",
    "df_corr = corr_pipeline.fit_transform(df)\n",
    "\n",
    "correlation_matrix = df_corr.corr()\n",
    "plt.figure(figsize= (30, 24))\n",
    "sns.heatmap(correlation_matrix, annot= True, cmap= 'coolwarm', fmt= '.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "correlation_with_target = df_corr.corr()['Target'].sort_values(ascending= False)\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_label = df_corr[df_corr['Target'] == 1]\n",
    "negative_label = df_corr[df_corr['Target'] == 0]\n",
    "\n",
    "for column in df_corr.columns[:-1]:\n",
    "    plt.figure(figsize= (5, 3))\n",
    "    sns.histplot(positive_label[column], bins= 15, color= 'b', kde= True, label= '1')\n",
    "    sns.histplot(negative_label[column], bins= 15, color= 'r', kde= True, label= '0')\n",
    "    plt.title(f\"Histogram of {column}:\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46145e1f",
   "metadata": {},
   "source": [
    "##### Detecting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fe6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pipeline = Pipeline([\n",
    "    (\"Feature Dropper\", FeatureDropper([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\",\n",
    "                                        #\"G\", \"H\", \"I\",\n",
    "                                        \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"\n",
    "                                       ])),\n",
    "])\n",
    "\n",
    "df_out = outlier_pipeline.fit_transform(df_train_x)\n",
    "plt.figure(figsize= (20, 10))\n",
    "sns.boxplot(data= df_out)\n",
    "sns.catplot(data= df_out)\n",
    "plt.xticks(rotation= 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b030dcf",
   "metadata": {},
   "source": [
    "### How well does the most important feature distinguish the target?\n",
    "##### The most important feature is taken as the one most correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GINI score of the best feature and the target in the whole dataset: {abs((2 * roc_auc_score(df['Target'], df['K']) - 1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83664b62",
   "metadata": {},
   "source": [
    "### How do different models perform using all features?\n",
    "##### Our baseline is 0.4600 GINI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_models(X_train, y_train, X_test, y_test, X_train_scaled, X_test_scaled,\n",
    "                    rf= True, xg= True, lr= True, mlp= True, random_grid= True, scoring= 'roc_auc', cv= 5):\n",
    "    # Random Forest\n",
    "    print(\"\\n\\033[1m\\033[4mRandom Forest results: \\033[0m\\n\")\n",
    "    rf_model = RandomForestClassifier(random_state= 42)\n",
    "    rf_param_grid = {'n_estimators': [50, 100, 200, 500],\n",
    "                     'max_depth': [None, 5, 10, 15],\n",
    "                     'min_samples_split': [2, 5, 8, 10],\n",
    "                     'min_samples_leaf': [1, 2, 4]}\n",
    "    train_test_model(X_train, y_train, X_test, y_test, rf_model, \n",
    "                     param_grid= rf_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if rf else None\n",
    "\n",
    "    # XGBoosting\n",
    "    print(\"\\n\\033[1m\\033[4mXGBoosting results: \\033[0m\\n\")\n",
    "    xgb_model = xgb.XGBClassifier(random_state= 42)\n",
    "    xgb_param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2], \n",
    "                      'max_depth': [3, 5, 7, 10], \n",
    "                      'min_child_weight': [1, 3, 5, 7], \n",
    "                      'gamma': [0.0, 0.1, 0.2, 0.3], \n",
    "                      'colsample_bytree': [0.3, 0.5, 0.8, 1.0], \n",
    "                      'reg_alpha': [0, 0.1, 1], \n",
    "                      'reg_lambda': [1, 1.5, 2]}\n",
    "    train_test_model(X_train, y_train, X_test, y_test, xgb_model, \n",
    "                     param_grid= xgb_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if xg else None\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print(\"\\n\\033[1m\\033[4mLogistic Regression results: \\033[0m\\n\")\n",
    "    lr_model = LogisticRegression(random_state= 42)\n",
    "    lr_param_grid = {'C': np.logspace(-4, 4, 20),\n",
    "                     'solver': ['lbfgs', 'newton-cholesky', 'newton-cg', 'sag', 'saga'],\n",
    "                     'max_iter': [750, 1000, 1500, 2000]}\n",
    "    train_test_model(X_train_scaled, y_train, X_test_scaled, y_test, lr_model,\n",
    "                     param_grid= lr_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if lr else None\n",
    "    \n",
    "    # Multi-Layer Perceptron\n",
    "    print(\"\\n\\033[1m\\033[4mMulti-Layer Perceptron results: \\033[0m\\n\")\n",
    "    mlp_model = MLPClassifier(random_state= 42)\n",
    "    mlp_param_grid = {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 25), (100, 50), (100, 100)],\n",
    "                      'activation': ['tanh', 'relu', 'logistic'],\n",
    "                      'solver': ['adam', 'sgd'],\n",
    "                      'alpha': uniform(0.0001, 0.01),\n",
    "                      'learning_rate': ['constant', 'adaptive']}\n",
    "    train_test_model(X_train_scaled, y_train, X_test_scaled, y_test, mlp_model,\n",
    "                     param_grid= mlp_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if mlp else None\n",
    "    \n",
    "def train_test_model(X_train, y_train, X_test, y_test, model, param_grid, random_grid, scoring, cv):\n",
    "    if random_grid:\n",
    "        grid = RandomizedSearchCV(estimator= model, param_distributions= param_grid, scoring= scoring, n_iter= 20, n_jobs= 4, cv= cv, verbose= 4)\n",
    "    else:\n",
    "        grid = GridSearchCV(estimator= model, param_grid= param_grid, scoring= scoring, n_jobs= 4, cv= cv, verbose= 4)\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    best_model = grid.best_estimator_\n",
    "    cv_results = grid.cv_results_\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print(f\"Best estimator: {best_model}\")\n",
    "    print(f\"Model Cross-Validation AUC: {cv_results['mean_test_score'][grid.best_index_]:.4f} with S.D. {cv_results['std_test_score'][grid.best_index_]}\\n\")\n",
    "    \n",
    "    y_pred_tr = best_model.predict(X_train)\n",
    "    y_prob_tr = best_model.predict_proba(X_train)[:, 1]\n",
    "    print(f\"Training Set Accuracy: {accuracy_score(y_train, y_pred_tr):.4f}\\nTraining Set AUC: {roc_auc_score(y_train, y_prob_tr):.4f}\")\n",
    "    print(f\"\\n\\033[1mTraining Set GINI: {(2 * roc_auc_score(y_train, y_prob_tr) - 1):.4f}\\033[0m\\n\")\n",
    "    \n",
    "    y_pred_ts = best_model.predict(X_test)\n",
    "    y_prob_ts = best_model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"Validation Set Accuracy: {accuracy_score(y_test, y_pred_ts):.4f}\\nValidation Set AUC: {roc_auc_score(y_test, y_prob_ts):.4f}\")\n",
    "    print(f\"\\n\\033[1mValidation Set GINI: {(2 * roc_auc_score(y_test, y_prob_ts) - 1):.4f}\\033[0m\\n\")\n",
    "    \n",
    "    print(f\"Prediction Report:\\n {classification_report(y_test, y_pred_ts)}\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_ts)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f685c",
   "metadata": {},
   "source": [
    "##### Trying with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allf_pipeline = Pipeline([\n",
    "    (\"Binary Encoder\", OneHotEncoding([\"A\"], binary= True)),\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([\"B\", \"C\", \"D\"])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([\"E\"], y_train))\n",
    "])\n",
    "scaling_pipeline = Pipeline([\n",
    "        (\"Amount Scalers\", CustomScaler([\"G\", \"H\", \"I\"], StandardScaler)),\n",
    "        (\"Count Scalers\", CustomScaler([\"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"], MinMaxScaler))\n",
    "])\n",
    "\n",
    "X_train = allf_pipeline.fit_transform(df_train_x)\n",
    "X_val = allf_pipeline.transform(df_val_x)\n",
    "X_train_scaled = scaling_pipeline.fit_transform(X_train)\n",
    "X_val_scaled = scaling_pipeline.transform(X_val)\n",
    "\n",
    "print_my_models(X_train, y_train, X_val, y_val, X_train_scaled, X_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6572136",
   "metadata": {},
   "source": [
    "##### Does performance increase when we drop uncorrelated features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold %1 or less\n",
    "dropper_pipeline_t1 = Pipeline([\n",
    "    (\"Feature Dropper\", FeatureDropper([\"B_1\", \"D_1\", \"D_2\", \"D_3\"]))\n",
    "])\n",
    "\n",
    "X_train_t1 = dropper_pipeline_t1.fit_transform(X_train)\n",
    "X_val_t1 = dropper_pipeline_t1.transform(X_val)\n",
    "X_train_scaled_t1 = dropper_pipeline_t1.fit_transform(X_train_scaled)\n",
    "X_val_scaled_t1 = dropper_pipeline_t1.transform(X_val_scaled)\n",
    "\n",
    "print_my_models(X_train_t1, y_train, X_val_t1, y_val, X_train_scaled_t1, X_val_scaled_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7cbac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Threshold %5 or less\n",
    "dropper_pipeline_t5 = Pipeline([\n",
    "    (\"Feature Dropper\", FeatureDropper([\"C_1\", \"L\", \"C_2\", \"D_6\",\n",
    "                                        \"B_1\", \"D_1\", \"D_2\", \"D_3\",\n",
    "                                        \"D_4\", \"B_2\", \"D_5\", \"B_3\"]))\n",
    "])\n",
    "\n",
    "X_train_t5 = dropper_pipeline_t5.fit_transform(X_train)\n",
    "X_val_t5 = dropper_pipeline_t5.transform(X_val)\n",
    "X_train_scaled_t5 = dropper_pipeline_t5.fit_transform(X_train_scaled)\n",
    "X_val_scaled_t5 = dropper_pipeline_t5.transform(X_val_scaled)\n",
    "\n",
    "print_my_models(X_train_t5, y_train, X_val_t5, y_val, X_train_scaled_t5, X_val_scaled_t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24115d0e",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corr(df, y, gini= False):\n",
    "    scores = {}\n",
    "    for feature in df.columns:\n",
    "        if gini:\n",
    "            score = 2 * roc_auc_score(y, df[feature]) - 1\n",
    "        else:\n",
    "            score = df[feature].corr(y)\n",
    "        scores[feature] = score\n",
    "    sorted_values = sorted(scores.items(), key= lambda item: item[1], reverse= True)\n",
    "    print(\"Scores:\")\n",
    "    for feature, score in sorted_values:\n",
    "        print(f\"{feature}:\\t{score}\")\n",
    "calculate_corr(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc8ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logtr_pipeline = Pipeline([\n",
    "    (\"Log\", LogTransformFeature([\"G\", \"H\", \"I\",\"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"])),\n",
    "    (\"Dropper\", FeatureDropper([\"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"]))\n",
    "])\n",
    "X_train_logtr = logtr_pipeline.fit_transform(X_train)\n",
    "X_val_logtr = logtr_pipeline.transform(X_val)\n",
    "calculate_corr(X_train_logtr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_my_models(X_train_logtr, y_train, X_val_logtr, y_val, X_train_logtr, X_val_logtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "denc_pipeline = Pipeline([\n",
    "    (\"Binary Encoder\", OneHotEncoding([\"A\"], binary= True)),\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([\"B\", \"C\"])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([\"D\", \"E\"], y_train))\n",
    "])\n",
    "X_train_denc = denc_pipeline.fit_transform(df_train_x)\n",
    "X_val_denc = denc_pipeline.transform(df_val_x)\n",
    "X_train_denc_scaled = scaling_pipeline.fit_transform(X_train_denc)\n",
    "X_val_denc_scaled = scaling_pipeline.transform(X_val_denc)\n",
    "print_my_models(X_train_denc, y_train, X_val_denc, y_val, X_train_denc_scaled, X_val_denc_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fced46",
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_pipeline = Pipeline([\n",
    "    (\"Interact\", InteractionFeature([ \"K\", \"L\", \"M\", \"G\"])),\n",
    "    (\"Scaler\", CustomScaler([\"K_x_L\", \"K_x_M\", \"K_x_G\", \"L_x_M\", \"L_x_G\", \"M_x_G\"], RobustScaler))\n",
    "])\n",
    "X_train_intr = intr_pipeline.fit_transform(X_train_denc)\n",
    "X_val_intr = intr_pipeline.transform(X_val_denc)\n",
    "X_train_intr_scaled = scaling_pipeline.fit_transform(X_train_intr)\n",
    "X_val_intr_scaled = scaling_pipeline.transform(X_val_intr)\n",
    "print_my_models(X_train_intr, y_train, X_val_intr, y_val, X_train_intr_scaled, X_val_intr_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logintr_pipeline = Pipeline([\n",
    "    (\"Log\", LogTransformFeature([\"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"])),\n",
    "    (\"Dropper\", FeatureDropper([\"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"U\", \"P\"])),\n",
    "    (\"Interact\", InteractionFeature([\"K_log\", \"L_log\", \"M_log\", \"G_log\"])),\n",
    "    (\"Scaler\", CustomScaler([\"K_log_x_L_log\", \"K_log_x_M_log\", \"K_log_x_G_log\", \"L_log_x_M_log\", \"L_log_x_G_log\", \"M_log_x_G_log\"], MinMaxScaler))])\n",
    "\n",
    "X_train_logintr = logintr_pipeline.fit_transform(X_train_denc)\n",
    "X_val_logintr = logintr_pipeline.transform(X_val_denc)\n",
    "print_my_models(X_train_logintr, y_train, X_val_logintr, y_val, X_train_logintr, X_val_logintr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac4be4",
   "metadata": {},
   "source": [
    "##### Trying out test data with the best models. (Highest GINI score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827db629",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = RandomForestClassifier(max_depth=10, min_samples_leaf=4, n_estimators=200, random_state=42)\n",
    "best_xgb_model = xgb.XGBClassifier(colsample_bytree=1.0, gamma=0.0, learning_rate=0.1, max_depth=3, min_child_weight=1,  random_state=42, reg_alpha= 1, reg_lambda= 2)\n",
    "best_lr_model = LogisticRegression(C=1438.44988828766, max_iter=2000, random_state=42, solver='sag')\n",
    "best_mlp_model = MLPClassifier(activation='logistic', alpha=0.0008645723603459421, hidden_layer_sizes=(100, 25), learning_rate='adaptive', random_state=42)\n",
    "\n",
    "def best_model_results(model, X_train, X_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_tr = model.predict(X_train)\n",
    "    y_prob_tr = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred_ts = model.predict(X_val)\n",
    "    end_time = time.time()\n",
    "    y_prob_ts = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    print(f\"Training Set Accuracy: {accuracy_score(y_train, y_pred_tr):.4f}\")\n",
    "    print(f\"\\n\\033[1mTraining Set GINI: {(2 * roc_auc_score(y_train, y_prob_tr) - 1):.4f}\\033[0m\\n\")\n",
    "\n",
    "    print(f\"Validation Set Accuracy: {accuracy_score(y_val, y_pred_ts):.4f}\")\n",
    "    print(f\"\\n\\033[1mValidation Set GINI: {(2 * roc_auc_score(y_val, y_prob_ts) - 1):.4f}\\033[0m\\n\")\n",
    "    \n",
    "    print(f\"Time taken for prediction: {(end_time - start_time):.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d372a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\033[1m\\033[4mRandom Forest results: \\033[0m\\n\")\n",
    "best_model_results(best_rf_model, X_train, X_val)\n",
    "\n",
    "print(\"\\n\\033[1m\\033[4mXGBoosting results: \\033[0m\\n\")\n",
    "best_model_results(best_xgb_model, X_train_denc, X_val_denc)\n",
    "\n",
    "print(\"\\n\\033[1m\\033[4mLogistic Regression results: \\033[0m\\n\")\n",
    "best_model_results(best_lr_model, X_train_logintr, X_val_logintr)\n",
    "\n",
    "print(\"\\n\\033[1m\\033[4mMulti-Layer Perceptron results: \\033[0m\\n\")\n",
    "best_model_results(best_mlp_model, X_train_logtr, X_val_logtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best Multi-Layer Perceptron model\n",
    "X_test_allf = allf_pipeline.transform(df_test)\n",
    "X_test = logtr_pipeline.transform(X_test_allf)\n",
    "\n",
    "y_pred = best_mlp_model.predict(X_test)\n",
    "y_prob = best_mlp_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = pd.DataFrame(y_pred, columns= ['Predictions'])\n",
    "y_prob = pd.DataFrame(y_prob, columns= ['Probabilities'])\n",
    "\n",
    "file_name = \"\"\n",
    "df_final = pd.read_excel(file_name)\n",
    "df_final = pd.concat([df_final, y_pred, y_prob], axis= 1)\n",
    "df_final.to_excel(file_name, index= False)\n",
    "print(\"Predictions and Probabilities saved to Test.xlsx.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
