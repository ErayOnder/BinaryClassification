{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc18ed01",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest, StackingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.stats import pointbiserialr, chi2_contingency, uniform, randint\n",
    "from itertools import combinations\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1b66d",
   "metadata": {},
   "source": [
    "## Pipeline Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_to_drop):\n",
    "        self.columns_to_drop = col_to_drop\n",
    "    \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns= self.columns_to_drop)\n",
    "\n",
    "class OneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, binary= False):\n",
    "        self.columns = columns\n",
    "        self.binary = binary\n",
    "        self.encoders = {}\n",
    "        self.new_column_names = {}\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            if self.binary:                \n",
    "                category_names = sorted(list(X_transformed[column].drop_duplicates()))\n",
    "                self.binary_check(column, category_names)\n",
    "                self.encoders[column] = {col: id for id, col in enumerate(category_names)}\n",
    "            else:\n",
    "                category_names = sorted([x for x in X_transformed[column].drop_duplicates() \n",
    "                                         if not (isinstance(x, float) and math.isnan(x))])\n",
    "                self.new_column_names[column] = [f\"{column}_{c}\" for c in category_names]\n",
    "                encoder = OneHotEncoder(dtype= np.int64, drop= 'if_binary')\n",
    "                encoder.fit(X_transformed[[column]])\n",
    "                self.encoders[column] = encoder\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            if self.binary:\n",
    "                X_transformed[column] = X_transformed[column].map(self.encoders[column])\n",
    "            else:\n",
    "                ohe_matrix = self.encoders[column].transform(X_transformed[[column]]).toarray()\n",
    "                if self.has_nan(X_transformed[column]):\n",
    "                    ohe_matrix = ohe_matrix[:, :-1]\n",
    "                for i, new_col in enumerate(self.new_column_names[column]):\n",
    "                    X_transformed.insert(X_transformed.columns.get_loc(column) + i, new_col, ohe_matrix[:, i])\n",
    "                X_transformed.drop(columns= [column], inplace= True)\n",
    "        return X_transformed\n",
    "    \n",
    "    def binary_check(self, column, categories):\n",
    "        if self.has_nan(categories):\n",
    "            raise ValueError(f\"Can't perform binary encoding to column {column} because there are NaN values.\")\n",
    "        if len(categories) > 2 or len(categories) == 0:\n",
    "            raise ValueError(f\"Can't perform binary encoding to column {column} because the number of categories to binary encode is wrong.\")\n",
    "\n",
    "    def has_nan(self, value_list):\n",
    "        return any([math.isnan(x) for x in value_list if isinstance(x, float)])\n",
    "\n",
    "class TargetMeanEncoding(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns, y, m= 10):\n",
    "        self.columns = columns\n",
    "        self.y_target = y\n",
    "        self.m = m\n",
    "        self.encoding_dict = {}\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        global_mean = self.y_target.mean()\n",
    "        for column in self.columns:\n",
    "            smoothed_mean = self.smooth_mean(X[column], global_mean)\n",
    "            self.encoding_dict[column] = dict(zip(X[column].dropna().drop_duplicates(), smoothed_mean.dropna().drop_duplicates()))        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        global_mean = self.y_target.mean()\n",
    "        \n",
    "        for column in self.columns:\n",
    "            if column in self.encoding_dict:\n",
    "                X_transformed[column] = X_transformed[column].map(self.encoding_dict[column]).fillna(global_mean)\n",
    "            else:\n",
    "                X_transformed[column] = X_transformed[column].fillna(global_mean)\n",
    "                \n",
    "        return X_transformed\n",
    "    \n",
    "    def smooth_mean(self, values, mean):\n",
    "        encoded_mean = self.y_target.groupby(values).mean()\n",
    "        counts = values.map(values.value_counts())\n",
    "        return (values.map(encoded_mean) * counts + mean * self.m) / (counts + self.m)\n",
    "\n",
    "class DataFrameImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, imputer_class, **imputer_kwargs):\n",
    "        self.imputer_class = imputer_class\n",
    "        self.imputer_kwargs = imputer_kwargs\n",
    "        self.imputer = self.imputer_class(**self.imputer_kwargs)\n",
    "    \n",
    "    def fit(self, X, y= None):\n",
    "        self.imputer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return pd.DataFrame(X_imputed, columns= X.columns, index= X.index)\n",
    "    \n",
    "class MulticollinearityEliminator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, y, threshold= 0.9):\n",
    "        self.y = y\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = None\n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        corr_matrix = X.corr().abs()\n",
    "        gini_scores = {}\n",
    "        for feature in X.columns:\n",
    "            gini = 2 * roc_auc_score(self.y, X[feature]) - 1\n",
    "            gini_scores[feature] = gini\n",
    "\n",
    "        to_drop = set()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                if corr_matrix.iloc[i, j] > self.threshold:\n",
    "                    feature_i = corr_matrix.columns[i]\n",
    "                    feature_j = corr_matrix.columns[j]\n",
    "\n",
    "                    if gini_scores[feature_i] > gini_scores[feature_j]:\n",
    "                        to_drop.add(feature_j)\n",
    "                    else:\n",
    "                        to_drop.add(feature_i)\n",
    "\n",
    "        self.to_drop = list(to_drop)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_reduced = X.drop(columns= self.to_drop)\n",
    "        return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbac4a5",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\", sep= ';')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4835b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Data\n",
    "df.set_index('ID', inplace= True)\n",
    "\n",
    "X, y = df.iloc[:, :-1], df.iloc[:,-1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size= 0.1, random_state= 42, stratify= y)\n",
    "\n",
    "print(\"\\n\\033[1mTraining Data information: \\033[0m\\n\")\n",
    "X_train.info()\n",
    "print(\"\\n\\033[1mTraining Label information: \\033[0m\\n\")\n",
    "y_train.info()\n",
    "label_count_tr = y_train.value_counts()\n",
    "print(\"\\n\\033[1mIn the Training Dataset:\\033[0m\\n\")\n",
    "print(f\"Percentage of positively labeled data: {((label_count_tr[1] / (label_count_tr[0] + label_count_tr[1])) * 100):.4f}%\")\n",
    "\n",
    "print(\"\\n\\033[1mValidation Data information: \\033[0m\\n\")\n",
    "X_val.info()\n",
    "print(\"\\n\\033[1mValidation Label information: \\033[0m\\n\")\n",
    "y_val.info()\n",
    "label_count_ts = y_val.value_counts()\n",
    "print(\"\\n\\033[1mIn the Validation Dataset: \\033[0m\\n\")\n",
    "print(f\"Percentage of positively labeled data: {(label_count_ts[1] / (label_count_ts[0] + label_count_ts[1])) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f29242",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29770d67",
   "metadata": {},
   "source": [
    "### Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c61d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = df[df['Target'] == 1]\n",
    "negative_labels = df[df['Target'] == 0]\n",
    "print(\"Null Check for positive labels: \\n\")\n",
    "sns.heatmap(positive_labels.isna(), cbar= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d73d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null Check for negative labels: \\n\")\n",
    "sns.heatmap(negative_labels.isna(), cbar= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d772c",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix_info(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize= (30, 24))\n",
    "    sns.heatmap(corr_matrix, annot= True, cmap= 'coolwarm', fmt= '.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    corr_with_target = df.corr()['Target'].sort_values(ascending= False)\n",
    "    print(corr_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc79c79",
   "metadata": {},
   "source": [
    "##### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = {}\n",
    "for feature in X.columns[:20]:\n",
    "    unique_categories[feature] = X[feature].nunique()\n",
    "    \n",
    "for feature, count in unique_categories.items():\n",
    "    print(f\"Feature {feature} has {count} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr_pipeline = Pipeline([\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([column for column, count in unique_categories.items() if count < 8])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([column for column, count in unique_categories.items() if count >= 8], y)),\n",
    "    (\"Feature Dropper\", FeatureDropper(X.columns[20:]))\n",
    "])\n",
    "df_cat_corr = cat_corr_pipeline.fit_transform(X)\n",
    "corr_matrix_info(pd.concat([df_cat_corr, y], axis= 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77308beb",
   "metadata": {},
   "source": [
    "##### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_corr_pipeline = Pipeline([\n",
    "    (\"Feature Dropper\", FeatureDropper(X.columns[:20]))\n",
    "])\n",
    "df_num_corr = num_corr_pipeline.fit_transform(X)\n",
    "corr_matrix_info(pd.concat([df_num_corr, y], axis= 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361107c",
   "metadata": {},
   "source": [
    "##### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr_pipeline = Pipeline([\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([column for column, count in unique_categories.items() if count < 8])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([column for column, count in unique_categories.items() if count >= 8], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean'))    \n",
    "])\n",
    "df_all_corr = all_corr_pipeline.fit_transform(X)\n",
    "corr_matrix_info(pd.concat([df_all_corr, y], axis= 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d81203",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pipeline = Pipeline([\n",
    "    (\"Feature Dropper\", FeatureDropper(X.columns[:20])),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean')),\n",
    "])\n",
    "df_out = outlier_pipeline.fit_transform(X)\n",
    "plt.figure(figsize= (20, 10))\n",
    "sns.boxplot(data= df_out)\n",
    "sns.catplot(data= df_out)\n",
    "plt.xticks(rotation= 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e05e3",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df, gini_scores, plot_type= 'histogram', threshold= 0.30):\n",
    "    filtered_features = [feature for feature, gini in gini_scores.items() if abs(gini) >= threshold]\n",
    "    if not filtered_features:\n",
    "        print(f\"No features with GINI scores greater than {threshold}.\")\n",
    "        return\n",
    "    \n",
    "    for feature in filtered_features:\n",
    "        plt.figure(figsize= (5, 3))\n",
    "        if plot_type == 'histogram':\n",
    "            sns.histplot(data= df, x= feature, hue= 'Target', kde= True, element= \"step\", stat= \"density\", common_norm= False)\n",
    "        elif plot_type == 'kde':\n",
    "            sns.kdeplot(data= df, x= feature, hue= 'Target', common_norm= False)\n",
    "        elif plot_type == 'boxplot':\n",
    "            sns.boxplot(data= df, x= 'Target', y= feature)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid plot_type: {plot_type}. Choose from 'histogram', 'kde', 'boxplot'.\")\n",
    "        plt.title(f\"Class Distribution for {feature} (Gini Score: {gini_scores[feature]:.4f})\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Density' if plot_type != 'boxplot' else 'Value')\n",
    "        plt.show()\n",
    "\n",
    "def plot_feature_scatter(df, gini_scores, pairs= None, threshold= 0.30):\n",
    "    filtered_features = [feature for feature, gini in gini_scores.items() if abs(gini) >= threshold]\n",
    "    if pairs is None:\n",
    "        pairs = [(filtered_features[i], filtered_features[j]) \n",
    "                 for i in range(len(filtered_features))\n",
    "                 for j in range(i+1, len(filtered_features))]\n",
    "    if not pairs:\n",
    "        print(f\"No features with GINI scores greater than {threshold}.\")\n",
    "        return\n",
    "    \n",
    "    for feature_x, feature_y in pairs:\n",
    "        if feature_x in df.columns and feature_y in df.columns:\n",
    "            plt.figure(figsize= (5, 3))\n",
    "            sns.scatterplot(data= df, x= feature_x, y= feature_y, hue= 'TARGET', palette= 'coolwarm', alpha= 0.7)\n",
    "            plt.title(f\"Scatter Plot of {feature_x} vs {feature_y}\")\n",
    "            plt.xlabel(feature_x)\n",
    "            plt.ylabel(feature_y)\n",
    "            plt.show()\n",
    "            \n",
    "def calculate_gini(df, y, prnt= False):\n",
    "    scores = {}\n",
    "    for feature in df.columns:\n",
    "        score = 2 * roc_auc_score(y, df[feature]) - 1\n",
    "        scores[feature] = score\n",
    "    if prnt:\n",
    "        sorted_values = sorted(scores.items(), key= lambda item: item[1], reverse= True)\n",
    "        print(\"Gini Scores:\")\n",
    "        for feature, score in sorted_values:\n",
    "            print(f\"{feature}:\\t{score}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "allf_pipeline = Pipeline([\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([column for column, count in unique_categories.items() if count < 8])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([column for column, count in unique_categories.items() if count >= 8], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean'))    \n",
    "])\n",
    "X_train_allf = allf_pipeline.fit_transform(X_train)\n",
    "gini_scores = calculate_gini(X_train_allf, y_train, prnt= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80badeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_allf_target = pd.concat([X_train_allf, y_train], axis= 1)\n",
    "plot_class_distribution(X_train_allf_target, gini_scores, plot_type= 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc35b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_scatter(X_train_allf_target, gini_scores, threshold= 0.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47260955",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ba2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_models(X_train, y_train, X_test, y_test,\n",
    "                    rf= True, xg= True, lr= True, random_grid= True, scoring= 'roc_auc', cv= 5):\n",
    "    # Random Forest\n",
    "    print(\"\\n\\033[1m\\033[4mRandom Forest results: \\033[0m\\n\")\n",
    "    rf_model = RandomForestClassifier(random_state= 42)\n",
    "    rf_param_grid = {'n_estimators': [50, 100, 200, 500],\n",
    "                     'max_depth': [None, 5, 10, 15],\n",
    "                     'min_samples_split': [2, 5, 8, 10],\n",
    "                     'min_samples_leaf': [1, 2, 4]}\n",
    "    train_test_model(X_train, y_train, X_test, y_test, rf_model, \n",
    "                     param_grid= rf_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if rf else None\n",
    "    \n",
    "    # XGBoosting\n",
    "    print(\"\\n\\033[1m\\033[4mXGBoosting results: \\033[0m\\n\")\n",
    "    xgb_model = xgb.XGBClassifier(random_state= 42)\n",
    "    xgb_param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2], \n",
    "                      'max_depth': [3, 5, 7, 10], \n",
    "                      'min_child_weight': [1, 3, 5, 7], \n",
    "                      'gamma': [0.0, 0.1, 0.2, 0.3], \n",
    "                      'colsample_bytree': [0.3, 0.5, 0.8, 1.0], \n",
    "                      'reg_alpha': [0, 0.1, 1], \n",
    "                      'reg_lambda': [1, 1.5, 2]}\n",
    "    train_test_model(X_train, y_train, X_test, y_test, xgb_model, \n",
    "                     param_grid= xgb_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if xg else None\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print(\"\\n\\033[1m\\033[4mLogistic Regression results: \\033[0m\\n\")\n",
    "    lr_model = LogisticRegression(random_state= 42)\n",
    "    lr_param_grid = {'C': np.logspace(-4, 4, 20),\n",
    "                     'solver': ['lbfgs', 'newton-cholesky', 'newton-cg', 'sag', 'saga'],\n",
    "                     'max_iter': [750, 1000, 1500, 2000, 3000, 5000]}\n",
    "    train_test_model(X_train, y_train, X_test, y_test, lr_model,\n",
    "                     param_grid= lr_param_grid, random_grid= random_grid, scoring= scoring, cv= cv) if lr else None\n",
    "\n",
    "def train_test_model(X_train, y_train, X_test, y_test, model, param_grid, random_grid, scoring, cv):\n",
    "    if random_grid:\n",
    "        grid = RandomizedSearchCV(estimator= model, param_distributions= param_grid, scoring= scoring, n_iter= 15, n_jobs= 4, cv= cv, verbose= 1)\n",
    "    else:\n",
    "        grid = GridSearchCV(estimator= model, param_grid= param_grid, scoring= scoring, n_jobs= 4, cv= cv, verbose= 4)\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    best_model = grid.best_estimator_\n",
    "    cv_results = grid.cv_results_\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print(f\"Best estimator: {best_model}\")\n",
    "    print(f\"Model Cross-Validation AUC: {cv_results['mean_test_score'][grid.best_index_]:.4f} with S.D. {cv_results['std_test_score'][grid.best_index_]}\\n\")\n",
    "    \n",
    "    y_prob_tr = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_tr, best_threshold = optimize_threshold(best_model, y_prob_tr, y_train)\n",
    "    print(f\"Training Set Accuracy: {accuracy_score(y_train, y_pred_tr):.4f}\\nTraining Set AUC: {roc_auc_score(y_train, y_prob_tr):.4f}\")\n",
    "    print(f\"\\n\\033[1mTraining Set GINI: {(2 * roc_auc_score(y_train, y_prob_tr) - 1):.4f}\\033[0m\\n\")\n",
    "    \n",
    "    y_prob_ts = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_ts = (y_prob_ts >= best_threshold).astype(int)\n",
    "    print(f\"Validation Set Accuracy: {accuracy_score(y_test, y_pred_ts):.4f}\\nValidation Set AUC: {roc_auc_score(y_test, y_prob_ts):.4f}\")\n",
    "    print(f\"\\n\\033[1mValidation Set GINI: {(2 * roc_auc_score(y_test, y_prob_ts) - 1):.4f}\\033[0m\\n\")\n",
    "    \n",
    "    print(f\"Prediction Report:\\n {classification_report(y_test, y_pred_ts)}\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_ts)}\\n\")\n",
    "\n",
    "def optimize_threshold(model, y_prob, y, thresholds= np.arange(0.0, 1.0, 0.05)):\n",
    "    best_threshold = 0.5\n",
    "    best_f1_score = 0.0\n",
    "    best_report = None\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        report = classification_report(y, y_pred, zero_division= 0, output_dict= True)\n",
    "        f1_minority = report['1']['f1-score']\n",
    "        if f1_minority > best_f1_score:\n",
    "            best_f1_score = f1_minority\n",
    "            best_threshold = threshold\n",
    "            best_report = report\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score for the minority class: {best_f1_score}\")\n",
    "    best_y_pred = (y_prob >= best_threshold).astype(int)\n",
    "    return best_y_pred, best_threshold\n",
    "\n",
    "def plot_dimensionality_reduction(X, y, dim= 2, label= \"PCA Projection\"):\n",
    "    pca = PCA(n_components= dim)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "                         \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    if dim == 2:\n",
    "        for l, c, m in zip(np.unique(y), colors, markers):\n",
    "            plt.scatter(X_reduced[y==l, 0], X_reduced[y==l, 1], c=c, label=l, marker=m)\n",
    "        plt.title(\"PCA 2D Projection\")\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "    elif dim == 3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection= '3d')\n",
    "        for l, c, m in zip(np.unique(y), colors, markers):\n",
    "            ax.scatter(X_reduced[y==l, 0], X_reduced[y==l, 1], X_reduced[y==l, 2], c=c, label=l, marker=m)\n",
    "        ax.set_title(\"PCA 3D Projection)\")\n",
    "        ax.set_xlabel(\"PCA Component 1\")\n",
    "        ax.set_ylabel(\"PCA Component 2\")\n",
    "        ax.set_zlabel(\"PCA Component 3\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "def sample_and_train(X_train, y_train, X_test, y_test, \n",
    "                     rus= True, tl= True, ros= True, smote= True, smoteenn= True):\n",
    "    # Under Samplers\n",
    "    if rus:\n",
    "        print(\"\\n\\033[1m\\033[4mPerforming Random Under Sampling... \\033[0m\\n\")\n",
    "        random_undersampler = RandomUnderSampler()\n",
    "        X_resampled_rus, y_resampled_rus = random_undersampler.fit_resample(X_train, y_train)\n",
    "        print_my_models(X_resampled_rus, y_resampled_rus, X_test, y_test)\n",
    "    if tl:\n",
    "        print(\"\\n\\033[1m\\033[4mPerforming Tomek Links Sampling... \\033[0m\\n\")\n",
    "        tomeklinks_undersampler = TomekLinks()\n",
    "        X_resampled_tl, y_resampled_tl = tomeklinks_undersampler.fit_resample(X_train, y_train)\n",
    "        print_my_models(X_resampled_tl, y_resampled_tl, X_test, y_test)\n",
    "        \n",
    "    # Over Samplers\n",
    "    if ros:\n",
    "        print(\"\\n\\033[1m\\033[4mPerforming Random Over Sampling... \\033[0m\\n\")\n",
    "        random_oversampler = RandomOverSampler()\n",
    "        X_resampled_ros, y_resampled_ros = random_oversampler.fit_resample(X_train, y_train)\n",
    "        print_my_models(X_resampled_ros, y_resampled_ros, X_test, y_test)\n",
    "    if smote:\n",
    "        print(\"\\n\\033[1m\\033[4mPerforming SMOTE Sampling... \\033[0m\\n\")\n",
    "        syn_oversampler = SMOTE()\n",
    "        X_resampled_smote, y_resampled_smote = syn_oversampler.fit_resample(X_train, y_train)\n",
    "        print_my_models(X_resampled_smote, y_resampled_smote, X_test, y_test)\n",
    "    \n",
    "    # Mix\n",
    "    if smoteenn:\n",
    "        print(\"\\n\\033[1m\\033[4mPerforming SMOTEENN Sampling... \\033[0m\\n\")\n",
    "        syn_underoversampler = SMOTEENN()\n",
    "        X_resampled_smoteenn, y_resampled_smoteenn = syn_underoversampler.fit_resample(X_train, y_train)\n",
    "        print_my_models(X_resampled_smoteenn, y_resampled_smoteenn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Encoding 1\n",
    "\n",
    "enc1_pipeline = Pipeline([\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([column for column, count in unique_categories.items() if count < 8])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([column for column, count in unique_categories.items() if count >= 8], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean'))\n",
    "])\n",
    "X_train_enc1 = enc1_pipeline.fit_transform(X_train)\n",
    "X_val_enc1 = enc1_pipeline.transform(X_val)\n",
    "\n",
    "#### Encoding 1 + M\n",
    "\n",
    "enc1m_pipeline = Pipeline([\n",
    "    (\"One-Hot Encoder\", OneHotEncoding([column for column, count in unique_categories.items() if count < 8])),\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding([column for column, count in unique_categories.items() if count >= 8], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean')),\n",
    "    (\"Multicollinearity\", MulticollinearityEliminator(y_train))\n",
    "])\n",
    "X_train_enc1m = enc1m_pipeline.fit_transform(X_train)\n",
    "X_val_enc1m = enc1m_pipeline.transform(X_val)\n",
    "\n",
    "#### Encoding 2\n",
    "\n",
    "enc2_pipeline = Pipeline([\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding(X_train.columns[:20], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean'))\n",
    "])\n",
    "X_train_enc2 = enc2_pipeline.fit_transform(X_train)\n",
    "X_val_enc2 = enc2_pipeline.transform(X_val)\n",
    "\n",
    "#### Encoding 2 + M\n",
    "\n",
    "enc2m_pipeline = Pipeline([\n",
    "    (\"Target-Mean Encoder\", TargetMeanEncoding(X_train.columns[:20], y)),\n",
    "    (\"Imputer\", DataFrameImputer(imputer_class= SimpleImputer, strategy= 'mean')),\n",
    "    (\"Multicollinearity\", MulticollinearityEliminator(y_train))\n",
    "])\n",
    "X_train_enc2m = enc2m_pipeline.fit_transform(X_train)\n",
    "X_val_enc2m = enc2m_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1079bd",
   "metadata": {},
   "source": [
    "#### Methods to handle the imbalanced dataset\n",
    "- Cost Sensitive Training\n",
    "- Under Sampling => Random Under Sampler, Tomek Links\n",
    "- Over Sampling => Random Over Sampler, SMOTE\n",
    "- Mix Sampling => SMOTEENN\n",
    "- One-Class Classification\n",
    "- Ensembling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c42151",
   "metadata": {},
   "source": [
    "##### Cost Sensitive Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c091f30",
   "metadata": {},
   "source": [
    "\"+ Loss\" fields in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cae63",
   "metadata": {},
   "source": [
    "RandomForest(..., class_weight= 'balanced')\n",
    "\n",
    "XGBClassifier(..., scale_pos_weight= negative_labels / positive_labels)\n",
    "\n",
    "LogisticRegression(..., class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d0af8",
   "metadata": {},
   "source": [
    "##### Under Sampling, Over Sampling, Mix Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335c59c",
   "metadata": {},
   "source": [
    "\"Samplers\" fields in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4418ee2",
   "metadata": {},
   "source": [
    "Under Samplers used: Random Under Sampler (RUS), Tomek Links (TL)\n",
    "\n",
    "Over Samplers used: Randm Over Sampler (ROS), SMOTE\n",
    "\n",
    "Mix Samplers used: SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimensionality_reduction(X_train_enc1, y_train, dim= 2, label= \"PCA Projection 2D\")\n",
    "plot_dimensionality_reduction(X_train_enc1, y_train, dim= 3, label= \"PCA Projection 3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\033[1m\\033[4mPerforming Random Under Sampling... \\033[0m\\n\")\n",
    "random_undersampler = RandomUnderSampler()\n",
    "X_resampled_rus, y_resampled_rus = random_undersampler.fit_resample(X_train_enc1, y_train)\n",
    "plot_dimensionality_reduction(X_resampled_rus, y_resampled_rus, dim= 2, label= \"PCA Projection 2D\")\n",
    "plot_dimensionality_reduction(X_resampled_rus, y_resampled_rus, dim= 3, label= \"PCA Projection 3D\")\n",
    "print(\"\\n\\033[1m\\033[4mPerforming Near Miss Sampling... \\033[0m\\n\")\n",
    "nearmiss_sampler = NearMiss()\n",
    "X_resampled_nms, y_resampled_nms = nearmiss_sampler.fit_resample(X_train_enc1, y_train)\n",
    "plot_dimensionality_reduction(X_resampled_nms, y_resampled_nms, dim= 2, label= \"PCA Projection 2D\")\n",
    "plot_dimensionality_reduction(X_resampled_nms, y_resampled_nms, dim= 3, label= \"PCA Projection 3D\")\n",
    "print(\"\\n\\033[1m\\033[4mPerforming Tomek Links Sampling... \\033[0m\\n\")\n",
    "tomeklinks_sampler = TomekLinks()\n",
    "X_resampled_tls, y_resampled_tls = tomeklinks_sampler.fit_resample(X_train_enc1, y_train)\n",
    "plot_dimensionality_reduction(X_resampled_tls, y_resampled_tls, dim= 2, label= \"PCA Projection 2D\")\n",
    "plot_dimensionality_reduction(X_resampled_tls, y_resampled_tls, dim= 3, label= \"PCA Projection 3D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd728b1",
   "metadata": {},
   "source": [
    "##### One-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minority = X_train_enc1[y_train == 1]\n",
    "\n",
    "## OneClassSVM\n",
    "ocsvm = OneClassSVM(kernel= 'rbf', gamma= 'auto', nu= 0.1)\n",
    "ocsvm.fit(X_minority)\n",
    "y_scores = ocsvm.decision_function(X_val_enc1)\n",
    "y_pred = (y_scores >= 0).astype(int)\n",
    "gini = 2 * roc_auc_score(y_val, y_scores) - 1\n",
    "print(\"For One Class SVM: \\n\")\n",
    "print(f\"Test set GINI score: {gini}\\n\")\n",
    "print(f\"Classification Report: \\n{classification_report(y_val, y_pred)}\\n\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_val, y_pred)}\\n\")\n",
    "\n",
    "## Isolation Forest\n",
    "isolation_forest = IsolationForest(n_estimators= 100, contamination= 0.1, random_state= 42)\n",
    "isolation_forest.fit(X_minority)\n",
    "y_scores = isolation_forest.decision_function(X_val_enc1)\n",
    "y_pred = (y_scores >= 0).astype(int)\n",
    "gini = 2 * roc_auc_score(y_val, y_scores) - 1\n",
    "print(\"For Isolation Forest: \\n\")\n",
    "print(f\"Test set GINI score: {gini}\\n\")\n",
    "print(f\"Classification Report: \\n{classification_report(y_val, y_pred)}\\n\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_val, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6bcdc",
   "metadata": {},
   "source": [
    "##### Ensembling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stacking Classifier\n",
    "\n",
    "param_grid = {\n",
    "    'final_estimator__C': np.logspace(-4, 4, 20),\n",
    "    'final_estimator__solver': ['lbfgs', 'newton-cholesky', 'newton-cg', 'sag', 'saga'],\n",
    "    'final_estimator__max_iter': [750, 1000, 1500, 2000, 3000, 5000]\n",
    "}\n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500, random_state=42)),\n",
    "    (\"xgb\", xgb.XGBClassifier(colsample_bytree=0.3, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=3,  random_state=42, reg_alpha= 1, reg_lambda= 1)),\n",
    "    (\"lr\", LogisticRegression(C=3792.690190732246, max_iter=5000, random_state=42))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "stacking_clf = StackingClassifier(estimators= base_models, final_estimator= meta_model, cv= 5, n_jobs= -1)\n",
    "train_test_model(X_train_enc1, y_train, X_val_enc1, y_val, stacking_clf, param_grid, random_grid= True, scoring= 'roc_auc', cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Voting Classifier\n",
    "\n",
    "param_grid = {\n",
    "    'rf__max_depth': [10], 'rf__min_samples_leaf': [4], 'rf__min_samples_split': [10], 'rf__n_estimators': [500], 'rf__random_state': [42],\n",
    "    'xgb__colsample_bytree': [0.3], 'xgb__gamma': [0.2], 'xgb__learning_rate': [0.05], 'xgb__max_depth': [5], 'xgb__min_child_weight': [3], 'xgb__reg_alpha': [1], 'xgb__reg_lambda': [1],\n",
    "    'lr__C': [3792.690190732246], 'lr__max_iter': [5000], 'lr__random_state': [42]\n",
    "}\n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier()),\n",
    "    (\"xgb\", xgb.XGBClassifier()),\n",
    "    (\"lr\", LogisticRegression())\n",
    "]\n",
    "voting_clf = VotingClassifier(estimators= base_models, voting= 'soft')\n",
    "train_test_model(X_train_enc1, y_train, X_val_enc1, y_val, voting_clf, param_grid= param_grid, random_grid= True, scoring= 'roc_auc', cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f0c82",
   "metadata": {},
   "source": [
    "### Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob_tr = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_tr, best_threshold = optimize_threshold(model, y_prob_tr, y_train)\n",
    "    print(f\"Training Set Accuracy: {accuracy_score(y_train, y_pred_tr):.4f}\\n\")\n",
    "    print(f\"\\n\\033[1mTraining Set GINI: {(2 * roc_auc_score(y_train, y_prob_tr) - 1):.4f}\\033[0m\\n\")\n",
    "\n",
    "    y_prob_ts = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_ts = (y_prob_ts >= best_threshold).astype(int)\n",
    "    print(f\"Validation Set Accuracy: {accuracy_score(y_test, y_pred_ts):.4f}\\n\")\n",
    "    print(f\"\\n\\033[1mValidation Set GINI: {(2 * roc_auc_score(y_test, y_prob_ts) - 1):.4f}\\033[0m\\n\")\n",
    "\n",
    "    print(f\"Prediction Report:\\n {classification_report(y_test, y_pred_ts)}\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred_ts)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd485cd",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6796840",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_labels = y_train.value_counts()[0]\n",
    "positive_labels = y_train.value_counts()[1]\n",
    "model = xgb.XGBClassifier(colsample_bytree=0.8, gamma=0.3, learning_rate=0.05, max_depth=3, min_child_weight=1,  random_state=42, reg_alpha= 1, reg_lambda= 2, scale_pos_weight= negative_labels / positive_labels)\n",
    "train_one_model(X_train_enc2m, y_train, X_val_enc2m, y_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0974eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(colsample_bytree=0.3, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=3,  random_state=42, reg_alpha= 1, reg_lambda= 1)\n",
    "train_one_model(X_train_enc1, y_train, X_val_enc1, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8c8ac",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=500, random_state=42)\n",
    "train_one_model(X_train_enc1, y_train, X_val_enc1, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718c536",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=29.763514416313132, max_iter=750, random_state=42, solver='newton-cg')\n",
    "tomeklinks_undersampler = TomekLinks()\n",
    "X_resampled_tl, y_resampled_tl = tomeklinks_undersampler.fit_resample(X_train_enc1, y_train)\n",
    "train_one_model(X_resampled_tl, y_resampled_tl, X_val_enc1, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f5a5",
   "metadata": {},
   "source": [
    "### Probabilities and Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fc7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"\", sep= ';')\n",
    "df_test.set_index('ID', inplace= True)\n",
    "\n",
    "negative_labels = y_train.value_counts()[0]\n",
    "positive_labels = y_train.value_counts()[1]\n",
    "xgb_best_model = xgb.XGBClassifier(colsample_bytree=0.8, gamma=0.3, learning_rate=0.05, max_depth=3, min_child_weight=1,  random_state=42, reg_alpha= 1, reg_lambda= 2, scale_pos_weight= negative_labels / positive_labels)\n",
    "xgb_best_model.fit(X_train_enc2m, y_train)\n",
    "\n",
    "y_prob_tr = xgb_best_model.predict_proba(X_train_enc2m)[:, 1]\n",
    "y_pred_tr, best_threshold = optimize_threshold(xgb_best_model, y_prob_tr, y_train)\n",
    "print(f\"Training Set Accuracy: {accuracy_score(y_train, y_pred_tr):.4f}\\n\")\n",
    "print(f\"\\n\\033[1mTraining Set GINI: {(2 * roc_auc_score(y_train, y_prob_tr) - 1):.4f}\\033[0m\\n\")\n",
    "\n",
    "X_test = enc2m_pipeline.transform(df_test)\n",
    "X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_test = xgb_best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = (y_prob_test >= best_threshold).astype(int)\n",
    "\n",
    "df_final = df_test.copy()\n",
    "df_final = df_final[[]]\n",
    "df_final[\"Probabilities\"] = y_prob_test\n",
    "df_final[\"Predictions\"] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"\", index= True)\n",
    "print(\"Predictions and Probabilities saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18ad10",
   "metadata": {},
   "source": [
    "###### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"\", sep= ';')\n",
    "target = test_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test GINI score: {2 * roc_auc_score(target, y_prob_test)- 1}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(target, y_pred_test)}\")\n",
    "print(f\"Prediction Report:\\n {classification_report(target, y_pred_test)}\")\n",
    "print(f\"Confusion Matrix:\\n {confusion_matrix(target, y_pred_test)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
